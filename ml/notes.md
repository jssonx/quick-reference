# Machine Learning

## MLE：最大似然估计（Maximimum Likelihood Estimate）
通过调整模型的参数，使得所有样本出现的可能性最大

似然：根据已知的结果，推测出现这个结果的可能环境，或者说是环境中的某些参数。也就是根据结果判断事情本身性质的过程，就是似然。

其中，Θ表示环境对应的参数，X表示事件发生的结果
 - P(X|Θ)表示根据Θ的环境下，事件X发生的概率。P是关于X的函数
 - L(Θ|X)表示已知观察结果是X的情况下，去推断Θ。L是关于Θ的函数

MLE：根据已知的样本标记结果，反推最大概率导致这些样本结果出现的模型参数

举例：抛硬币。假设P(人像朝上)=Θ，(数字朝上)=1-Θ。Θ是存在的，单我们不知道Θ具体是多少。这需要根据观测结果进行推断。为了获得Θ，进行抛硬币实验，并记录结果。比如结果序列为7次人像朝上，3次数字朝上，那么L(Θ)=Θ^7(1-Θ)^3。函数L称为Θ的似然函数。最大似然估计，就是要求Θ等于多少的时候似然函数最大。也就是在横坐标为Θ，纵座标为L(Θ)，求L最高的时候，Θ的值。

## MAP：最大后验估计
通过调整模型的参数，使得所有样本出现的可能性最大，但是模型参数服从某个概率分布，换句话说每个模型参数的可能性（先验）不一样；在MLE中，每个模型参数出现的可能性默认都一样

## SVM：支持向量机
要寻找一个分解面M，将全部样本正确分成两类，并且样本和分解面之间的间隔需要是最大的

全部样本X=(x1,x2,...,xn)，其中xi=(xi1,xi2,...,xid)，xi1,xi2,...,xid表示样本的特征，xi属于R^d。法向量W=(w1,w2,...,wd)，w0表示偏移量。分解面M的方程为：W^T * xi + w0 = 0，或W^T * xi + b = 0，或w1x1+w2x2...+w2x2+b=0。如果空间中有m个样本，将任意的第i个样本表示为(x^(i),y^(i))，其中x^(i)是特征向量，y^(i)是样本标记，i表示样本编号。由于是分类问题，当样本是正例的时候，样本标记y=+1；当样本是负例的时候，样本标记y=-1。将第i个样本的预测值表示为WTx^(i)+b。

SVM算法将尝试寻找一个最优的决策平面，该平面距离两个类别的最近样本最远，假设第i个样本到平面的距离为distance(x^(i),W)，这里的W代表了分界面的全部参数。我们需要计算出全部样本到分界面的距离。假设这些距离中最短的长度是d，d=min distance(x^(i),W)，那么间隔margin的长度就定义为2d。而SVM需要求出来最大margin的分界面参数W

空间中点到平面的距离公式：distance(x^(i),W)=|W^T * x^(i) + b| / ||W||，其中||W||表示W的模（根号下平方和）

又因为d是样本到分界面的最短距离，所以对于任意正样本，即任意y^(i)=1，有distance(x^(i),W)≥d；对于任意负样本，即任意y^(i)=1，有distance(x^(i),W)≤-d。

margin的长度为2d=2/||W||

如果某个向量到分界面的距离恰好为d，那么这个向量就是支持向量。支撑向量决定了分界面的间隔margin的大小

为了使margin最大，需要最小化||W||，即最小化W的模。也就是说求最大化margin，要计算最小化的||w||。margin=2d=2/||W||，所以最大化margin，就是最小化||W||，也就是min (1/2) * ||W||^2。而(1/2) * ||W||^2=1/2 * (w1^2+w2^2+...+wd^2)=1/2 * W^T * W

所以SVM的目标是，在y^(i)(WTX^(i)+b)≥1的前提下，求最小min(1/2)WT*W时，得到的W=(w1, w2, ..., wn)的结果